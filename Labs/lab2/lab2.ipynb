{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69eeaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my version\n",
    "from stemming.porter2 import stem\n",
    "import nltk\n",
    "import os\n",
    "import bisect # increase insert efficiency\n",
    "# store the stop words as set.\n",
    "with open(\"englishST.txt\",\"r\") as file:\n",
    "    STOPWORDS=set(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(filename):\n",
    "    file_content = open(filename).read()\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_content) # tokenisation\n",
    "    # case folding(convert text into lower case)\n",
    "    # remove stop words\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() and token not in STOPWORDS]\n",
    "    tokens = [stem(token) for token in tokens] # normalisation:stem\n",
    "    filename = 'output_'+filename[:-4]+'.txt'\n",
    "    f = open(filename, \"a\")\n",
    "    f.write(\"\\n\".join(tokens))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "205b4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return stemmed_words is a list of tuples(word, docID, position)\n",
    "def preprocess(text, document_number):\n",
    "    # tokenisation\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    # case folding(convert text into lower case)\n",
    "    # remove non-letter and non-digit\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha() or token.isdigit()]\n",
    "    \n",
    "#     # remove english stopwords\n",
    "#     tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    \n",
    "    # # normalisation:stemming\n",
    "    stemmed_words = [stem(token) for token in tokens]\n",
    "\n",
    "    words_and_positions = list(\n",
    "                    zip(stemmed_words, [document_number] * len(stemmed_words), range(1, len(stemmed_words) + 1)))\n",
    "\n",
    "    # stemmed_words is a list of tuples(word, docID, position)\n",
    "    # where elem = word, docID\n",
    "    return words_and_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a749afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ElementTree\n",
    "import re\n",
    "from stemming.porter2 import stem\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# xml file to text version\n",
    "def xmlTotext(filename):\n",
    "    tree = ElementTree.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    filename = filename[:-4] + '_output' + '.txt'\n",
    "    fil = open(filename, \"a\")\n",
    "    for childs in root:\n",
    "        for i in range(len(childs)):\n",
    "            if childs[i].tag == 'DOCNO':\n",
    "                fil.write(\"ID: \" + childs[i].text.strip()+'\\n')\n",
    "            elif childs[i].tag == 'HEADLINE':\n",
    "                fil.write(\"HEADLINE: \"+ childs[i].text.strip()+'\\n')\n",
    "            elif childs[i].tag == 'TEXT':\n",
    "                fil.write(\"TEXT: \"+ childs[i].text.strip()+'\\n')\n",
    "    fil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6a081418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "postings = dict()\n",
    "term_frequency = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488acd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_posting(document_number, position):\n",
    "\n",
    "    if document_number in postings.keys():\n",
    "        bisect.insort(postings[document_number], position)\n",
    "        self.term_frequency[document_number] += 1\n",
    "    else:\n",
    "        self.postings[document_number] = [position]\n",
    "        self.term_frequency[document_number] = 1\n",
    "        self.add_appearance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ff2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(self, input_file):\n",
    "    \"\"\"\n",
    "    :param input_file: original XML file\n",
    "    :return: dictionary of words and information about them, such as the document frequency and postings\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_file, 'r') as f:  # Reading file\n",
    "        xml = f.read()\n",
    "\n",
    "        xml = '<ROOT>' + xml + '</ROOT>'  # Let's add a root tag\n",
    "\n",
    "        root = ElementTree.fromstring(xml)\n",
    "        word_list = []\n",
    "\n",
    "        # Simple loop through each document\n",
    "        for doc in root:\n",
    "            for elem in doc:\n",
    "\n",
    "                if elem.tag.lower() == 'docno':\n",
    "                    document_number = int(elem.text.strip())\n",
    "\n",
    "\n",
    "                elif elem.tag.lower() == 'headline':\n",
    "                    headline = elem.text.strip()\n",
    "\n",
    "                    # preprocess the headline\n",
    "                    preprocessed_headline = self.preprocess(headline, document_number)\n",
    "                    word_list.extend(preprocessed_headline)\n",
    "\n",
    "                elif elem.tag.lower() == 'text':\n",
    "                    text = elem.text.strip()\n",
    "\n",
    "                    # preprocess the text\n",
    "                    # this returns a list of tuples\n",
    "                    # (word, document_number, position)\n",
    "                    # positions counted after stopword removal\n",
    "                    preprocessed_text = self.preprocess(text, document_number)\n",
    "                    word_list.extend(preprocessed_text)\n",
    "\n",
    "    # sort the word list by word\n",
    "    # then by document number\n",
    "    # appearances should already be ordered\n",
    "    \n",
    "    \n",
    "    word_list = sorted(word_list)\n",
    "\n",
    "    # merge into postings list\n",
    "    for elem in word_list:\n",
    "        word = elem[0]\n",
    "        document_number = elem[1]\n",
    "        position = elem[2]\n",
    "\n",
    "        new_term = Term(word)\n",
    "\n",
    "        if word in self.index.keys():\n",
    "            # the index has seen this word before\n",
    "            # we update the number of occurrences\n",
    "            # and the postings list\n",
    "            term = self.index[word]\n",
    "            term.add_posting(document_number, position)\n",
    "\n",
    "        else:\n",
    "            # print(\"Word not in index!\")\n",
    "            new_term.add_posting(document_number, position)\n",
    "            self.index[word] = new_term\n",
    "\n",
    "    return self.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
